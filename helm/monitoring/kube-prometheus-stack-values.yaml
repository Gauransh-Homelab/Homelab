# Enhanced kube-prometheus-stack values for Talos Linux homelab
# Optimized for 2-node cluster with room to grow
# Control plane IPs: 192.168.10.147 (master), 192.168.10.165 (worker)

namespaceOverride: monitoring

# Global settings
global:
  rbac:
    create: true
    createAggregateClusterRoles: true

# Prometheus configuration - production-ready
prometheus:
  prometheusSpec:
    replicas: 1  # Will increase to 2 when cluster grows
    retention: 15d
    retentionSize: 45GB
    
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: synology-iscsi
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 2000m
        memory: 4Gi
    
    # Enable features for modern monitoring
    enableFeatures:
      - memory-snapshot-on-shutdown
    
    # Enable remote write receiver endpoint
    enableRemoteWriteReceiver: true
    
    # Additional alert relabel configs to suppress false positives
    additionalAlertRelabelConfigs:
      # Drop alerts for Talos control plane components that aren't exposed
      - source_labels: [alertname]
        regex: '(KubeControllerManagerDown|KubeSchedulerDown|etcdInsufficientMembers|etcdMembersDown)'
        action: drop
      # Drop TargetDown alerts for known Talos limitations
      - source_labels: [alertname, job]
        regex: 'TargetDown;(kube-controller-manager|kube-scheduler|kube-etcd)'
        action: drop
      # Drop TargetDown alerts for prometheus.scrape.* jobs (Alloy internal)
      - source_labels: [alertname, job]
        regex: 'TargetDown;prometheus\.scrape\..*'
        action: drop
    
    # Additional scrape configs for Talos nodes
    additionalScrapeConfigs:
      - job_name: 'talos-nodes'
        static_configs:
          - targets: ['192.168.10.147:9100', '192.168.10.165:9100']
    
    # Pod and service monitors
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false

# Grafana - feature-rich configuration
grafana:
  enabled: true
  replicas: 1
  
  persistence:
    enabled: true
    storageClassName: synology-iscsi
    size: 10Gi
  
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  service:
    type: LoadBalancer
  
  # Enable useful plugins
  plugins:
    - grafana-piechart-panel
    - grafana-clock-panel
    - natel-discrete-panel
  
  # Pre-configure data sources
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki:3100
      isDefault: false
    - name: AlertManager
      type: alertmanager
      url: http://kube-prometheus-stack-alertmanager:9093
      access: proxy

  # Pre-load dashboards
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: false
        updateIntervalSeconds: 30
        options:
          path: /var/lib/grafana/dashboards/kubernetes
      - name: 'talos'
        orgId: 1
        folder: 'Talos'
        type: file
        disableDeletion: false
        updateIntervalSeconds: 30
        options:
          path: /var/lib/grafana/dashboards/talos
  
  dashboards:
    kubernetes:
      k8s-cluster:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      k8s-nodes:
        gnetId: 15520
        datasource: Prometheus
      k8s-resources:
        gnetId: 13332
        datasource: Prometheus
    talos:
      talos-linux:
        gnetId: 19122
        datasource: Prometheus

# AlertManager - enabled for future use
alertmanager:
  enabled: true
  config:
    existingSecret: alertmanager-config
    existingSecretKey: alertmanager.yaml
  alertmanagerSpec:
    replicas: 1
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: synology-iscsi
          resources:
            requests:
              storage: 2Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi

# Prometheus Operator
prometheusOperator:
  enabled: true
  admissionWebhooks:
    enabled: false  # Skip for simplicity
    failurePolicy: Ignore
    createSecretJob: false  # Also disable secret creation job
    patch:
      enabled: false  # Disable patching webhooks
    certManager:
      enabled: false  # Not using cert-manager
  # Disable TLS for the operator
  tls:
    enabled: false
  # Don't use admission webhook port
  hostNetwork: false
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  # Monitor the operator itself
  serviceMonitor:
    selfMonitor: true
    # Disable TLS for service monitor
    metricRelabelings: []
    relabelings: []
    scheme: http
    tlsConfig:
      insecureSkipVerify: true

# ENABLE: Node exporter for comprehensive node metrics
nodeExporter:
  enabled: true
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi

# ENABLE: kube-state-metrics for rich K8s object metrics
kubeStateMetrics:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 300m
      memory: 256Mi

# ENABLE: Talos control plane monitoring (API server only)
kubeApiServer:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      - sourceLabels: [__name__]
        regex: 'apiserver_request_duration_seconds.*'
        action: keep

# DISABLE: Not accessible in Talos Linux
kubeControllerManager:
  enabled: false

kubeScheduler:
  enabled: false

kubeEtcd:
  enabled: false

# ENABLE: CoreDNS monitoring
coreDns:
  enabled: true
  service:
    enabled: true

# ENABLE: Kubelet monitoring on all nodes
kubelet:
  enabled: true
  serviceMonitor:
    cAdvisor: true
    probes: true
    resource: true

# DISABLE: Talos uses cilium, not kube-proxy
kubeProxy:
  enabled: false

# Default recording rules and alerts
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: false  # Disable - Talos manages etcd internally
    general: true
    k8s: true
    kubeApiserver: true
    kubeApiserverAvailability: true
    kubeApiserverError: true
    kubeApiserverSlos: true
    kubelet: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeScheduler: false  # Disable - Talos scheduler not directly accessible
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
  
  # Disable specific alerts that don't apply to Talos
  disabled:
    KubeControllerManagerDown: true
    KubeSchedulerDown: true
    etcdInsufficientMembers: true
    etcdMembersDown: true
    KubeCPUOvercommit: true  # Normal for homelab
    KubeMemoryOvercommit: true  # Normal for homelab